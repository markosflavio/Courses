{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice 2 - Building a 2nd-order language model from text databases\n",
    "\n",
    "Markos Flavio B. G. O.\n",
    "\n",
    "__Context: Markov Models (MMs).__\n",
    "\n",
    "__Course: Unsupervised Machine Learning Hidden Markov Models in Python (Udemy, LazyProgrammer)__\n",
    "\n",
    "This code is a practice study about MMs. It's an adaptation of the code found at https://github.com/lazyprogrammer/machine_learning_examples/tree/master/hmm_class/frost.py\n",
    "    \n",
    "__Specific objectives__\n",
    "\n",
    "     1. Build a 2nd-order language model from a text database\n",
    "     2. Generate new phrases from the learned model\n",
    "     3. Calculating the probability of a given phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, we want to count:\n",
    "1. The initial distribution of words (probability of words appearing at the beggining of a sentence).\n",
    "2. Second word distribution (won't have two previous words here; we could include a None in the first position (w(t-2))).\n",
    "3. End of sentence distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a 2nd-order language model from a text database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Two roads diverged in a yellow wood,\n",
      "\n",
      "1 And sorry I could not travel both\n",
      "\n",
      "2 And be one traveler, long I stood\n",
      "\n",
      "3 And looked down one as far as I could\n",
      "\n",
      "4 To where it bent in the undergrowth; \n",
      "\n",
      "5 \n",
      "\n",
      "6 Then took the other, as just as fair,\n",
      "\n",
      "7 And having perhaps the better claim\n",
      "\n",
      "8 Because it was grassy and wanted wear,\n",
      "\n",
      "9 Though as for that the passing there\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# looking at the data\n",
    "loc = './Raw repo/hmm_class/robert_frost.txt'\n",
    "for i, line in enumerate(open(loc)):\n",
    "    if i < 10:\n",
    "        print(i, line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 two roads diverged in a yellow wood,\n",
      "1 and sorry i could not travel both\n",
      "2 and be one traveler, long i stood\n",
      "3 and looked down one as far as i could\n",
      "4 to where it bent in the undergrowth;\n",
      "6 then took the other, as just as fair,\n",
      "7 and having perhaps the better claim\n",
      "8 because it was grassy and wanted wear,\n",
      "9 though as for that the passing there\n"
     ]
    }
   ],
   "source": [
    "# removing additional spaces and putting all characters into lower case\n",
    "for i, line in enumerate(open(loc)):\n",
    "    if i < 10 and line.strip(): # removing empty lines\n",
    "        print(i, line.rstrip().lower())       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 two roads diverged in a yellow wood\n",
      "1 and sorry i could not travel both\n",
      "2 and be one traveler long i stood\n",
      "3 and looked down one as far as i could\n",
      "4 to where it bent in the undergrowth\n",
      "6 then took the other as just as fair\n",
      "7 and having perhaps the better claim\n",
      "8 because it was grassy and wanted wear\n",
      "9 though as for that the passing there\n"
     ]
    }
   ],
   "source": [
    "# removing punctuation\n",
    "for i, line in enumerate(open(loc)):\n",
    "    if i < 10 and line.strip():\n",
    "        print(i, line.rstrip().lower().translate(str.maketrans('','',string.punctuation)))\n",
    "        # translate() returns a string where each character is mapped to its corresponding character as per the translation table.\n",
    "        # the translation table is created by the maketrans() method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now build an array of all possible values for any sequence of two words in the dataset.\n",
    "\n",
    "Example: if in the dataset we have three sentences like \"I love dogs\", \"I love cats\", \"I love dogs\", \"I love\", we would the following dictionary:\n",
    "\n",
    "{('I', 'love'): ['dogs', 'cats', dogs, 'END']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = {} # holds the counting of words as being the firts one in the sentence\n",
    "second_word = {} # holds the counting of words as being the 2nd one (they only have one previous word)\n",
    "transitions = {}  # holds all 2nd-order transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add2dict(d, k, v):\n",
    "    if k not in d:\n",
    "        d[k] = []\n",
    "    d[k].append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['two', 'roads', 'diverged', 'in', 'a', 'yellow', 'wood']\n",
      "Example of transition:\n",
      "{('two', 'roads'): ['diverged'], ('roads', 'diverged'): ['in'], ('diverged', 'in'): ['a'], ('in', 'a'): ['yellow'], ('yellow', 'wood'): ['END'], ('a', 'yellow'): ['wood']}\n"
     ]
    }
   ],
   "source": [
    "# gathering data\n",
    "for i, line in enumerate(open(loc)):\n",
    "    tokens = line.rstrip().lower().translate(str.maketrans('','',string.punctuation)).split()\n",
    "    T = len(tokens)\n",
    "    \n",
    "    for j in range(T):\n",
    "        t = tokens[j]\n",
    "        if j == 0:\n",
    "            initial[t] = initial.get(t, 0.) + 1\n",
    "        else:\n",
    "            t_1 = tokens[j-1]\n",
    "            if j == T - 1:\n",
    "                add2dict(transitions, (t_1, t), 'END')\n",
    "            if j == 1:\n",
    "                add2dict(second_word, t_1, t)\n",
    "            else:\n",
    "                t_2 = tokens[j-2]\n",
    "                add2dict(transitions, (t_2, t_1), t)\n",
    "    \n",
    "    if i == 0: # looking at a sample\n",
    "        print(tokens)\n",
    "        print('Example of transition:')\n",
    "        print(transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('stood', 'the'): ['strain']}\n",
      "{'said': ['that']}\n",
      "{'brown': 2.0}\n"
     ]
    }
   ],
   "source": [
    "# looking at the some sample of the data\n",
    "def print_sample(d):\n",
    "    print(dict(random.sample(d.items(), 1)))\n",
    "print_sample(transitions)\n",
    "print_sample(second_word)\n",
    "print_sample(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning each array into a probability distribution dicionary.\n",
    "\n",
    "In the early example the array ['dogs', 'cats', dogs, 'END'] becomes {\"cats\": 1/4, \"dogs\": 1/2, END: 1/4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the 'initial' array is easy\n",
    "initial_total = sum(initial.values())\n",
    "for t, c in initial.items():\n",
    "    initial[t] = c / initial_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sitting': 0.0006963788300835655}\n"
     ]
    }
   ],
   "source": [
    "print_sample(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2pdict(ts):\n",
    "    # function to turn each list of possibilities into a dictionary of probabilities\n",
    "    d = {}\n",
    "    n = len(ts)\n",
    "    # counting\n",
    "    for t in ts:\n",
    "        d[t] = d.get(t, 0.) + 1\n",
    "        # normalizing\n",
    "    for t, c in d.items():\n",
    "        d[t] = c / n\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in second_word.items():\n",
    "    second_word[k] = list2pdict(v)\n",
    "for k, v in transitions.items():\n",
    "    transitions[k] = list2pdict(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('hard', 'with'): {'john': 1.0}}\n",
      "{'making': {'allowance': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "print_sample(transitions)\n",
    "print_sample(second_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating new phrases given the learned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling the dictionary a random word using the distribution found based on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_word(d):\n",
    "    p0 = np.random.random()\n",
    "    cumulative = 0\n",
    "    for t, p in d.items():\n",
    "        cumulative += p\n",
    "        if p0 < cumulative:\n",
    "            return t\n",
    "    assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_sentences(n):\n",
    "    for i in range(n):\n",
    "        sentence  = []\n",
    "        # initial word\n",
    "        w0 = sample_word(initial)\n",
    "        sentence.append(w0)\n",
    "        \n",
    "        # sample second word\n",
    "        w1 = sample_word(second_word[w0])\n",
    "        sentence.append(w1)\n",
    "\n",
    "        # second-order transitions until END\n",
    "        while True:\n",
    "            w2 = sample_word(transitions[(w0, w1)])\n",
    "            if w2 == 'END':\n",
    "                break\n",
    "            sentence.append(w2)\n",
    "            w0 = w1\n",
    "            w1 = w2\n",
    "        print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in snow and mist\n",
      "to be you said youd seen the stone baptismal font outdoors\n",
      "and this is the ideals\n",
      "i made him keep on gnawing till he whined\n",
      "it ought to know\n"
     ]
    }
   ],
   "source": [
    "generating_sentences(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the probability of a given sentece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To include smoothing, the probability distributions must be computed again from the new counts. It would also be better to check whether the sentence has only words included in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08983286908077995\n",
      "0.007751937984496124\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0006963788300835655"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prob_seq_no_smoothing(sentence):\n",
    "    \n",
    "    # preprocessing\n",
    "    sentence = sentence.rstrip().lower().translate(str.maketrans('','',string.punctuation))\n",
    "    # splitting\n",
    "    seq = sentence.split()\n",
    "\n",
    "    # evaluating the probabilities\n",
    "    try:\n",
    "        prob1 = initial[seq[0]] # prob. of 1st word\n",
    "        print(prob1)\n",
    "    except KeyError: # not found in the database as 1st word\n",
    "        return 0\n",
    "    try:\n",
    "        prob2 = second_word[seq[0]][seq[1]] # prob. of 2nd word\n",
    "        print(prob2)\n",
    "    except KeyError:\n",
    "        return 0\n",
    "    \n",
    "    if len(sentence) > 2:\n",
    "        w2 = seq[0]; w1 = seq[1]\n",
    "        prob = 1\n",
    "        for word in seq[2:]:\n",
    "            try:\n",
    "                prob = prob*transitions[(w2, w1)][word]\n",
    "                print(prob)\n",
    "            except KeyError:\n",
    "                return 0\n",
    "            w2 = w1\n",
    "            w1 = word\n",
    "        return prob1*prob2*prob\n",
    "    else:\n",
    "        return prob1*prob2\n",
    "    \n",
    "prob_seq_no_smoothing('and having perhaps')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
